{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduktion til Airflow\n",
    "\n",
    "Data engineering er at gør data troværdig, gentagende og maintable process.\n",
    "\n",
    "Workflow er skridt i a en process som at downloade filer osv. \n",
    "\n",
    "Airflow bruges til at lave et workflow. Disse workflows kaldes `DAGs (Directed Acyclic Graphs)`.\n",
    "\n",
    "DAgs er sæt af opgaver i et workflow. \n",
    "\n",
    "For ar kære et worfkflow kan vi bruge\n",
    "\n",
    "```\n",
    "airflow run <dag_id> <task_id> <start_date>\n",
    "```\n",
    "\n",
    "En mulig error vil være `airflow.exceptions.AirflowException`.\n",
    "\n",
    "Ved at køre `airflow` kan man se en række komando strenge:\n",
    "\n",
    "```\n",
    "repl:~$ airflow\n",
    "[2022-06-28 19:49:23,572] {__init__.py:51} INFO - Using executor SequentialExecutor\n",
    "usage: airflow [-h]\n",
    "               {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key}\n",
    "               ...\n",
    "airflow: error: the following arguments are required: subcommand\n",
    "repl:~$\n",
    "```\n",
    "\n",
    "## Airflow DAGs\n",
    "\n",
    "Directed Acyclic graph:\n",
    "\n",
    "Directed der er en flow der udviser afhænge mellem komponenter.\n",
    "\n",
    "Acycluc, betyder at der ikke rer loop og den kører engang.\n",
    "\n",
    "grapch de række komponenter. \n",
    "\n",
    "Python bruges til at definere DAGs.\n",
    "\n",
    "tasks er de opgaver DAGs skal køre. \n",
    "\n",
    "Afhængigheder mellem tasks er essentiel. \n",
    "\n",
    "Dag er case centitive.\n",
    "\n",
    "Hvis man skal bruge hjælp kan man skrive.\n",
    "\n",
    "```\n",
    "airflow -h\n",
    "```\n",
    "\n",
    "Kommandoer til at vise hvordan man kan burge airflow.\n",
    "\n",
    "```\n",
    "airflow list_dags\n",
    "```\n",
    "\n",
    "En simpel DAG\n",
    "\n",
    "```\n",
    "# Import the DAG object\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG('example_etl', default_args=default_args)\n",
    "```\n",
    "\n",
    "## Airflow web interface\n",
    "\n",
    "Her kan man se dags og hvordan processen er. \n",
    "\n",
    "Det giver os et DAG detail view.\n",
    "\n",
    "I view får vi et chart for at have et overblik omkring data. \n",
    "\n",
    "VI kan også se hvordan python koden er definerede. Den er kun read only. \n",
    "\n",
    "Undeer Browse kan vi se Logs. \n",
    "\n",
    "Vi kan også burge kommando til at se det samme.\n",
    "\n",
    "Start en webserver\n",
    "\n",
    "```\n",
    "airflow webserver -p 1010\n",
    "```\n",
    "\n",
    "\n",
    "# Implementer Airflow DAGs\n",
    "\n",
    "## Airflow operator\n",
    "\n",
    "operator er en enkelt task i workflow.\n",
    "Køre uafhængigt. \n",
    "\n",
    "\n",
    "`BashOperator` en bash kommando og kræver tre argument\n",
    "\n",
    "```\n",
    "BashOperator(\n",
    "  task_id = 'bash_example',\n",
    "  bash_command = 'echo \"Example!\"',\n",
    "  dag = ml_dag\n",
    ")\n",
    "```\n",
    "\n",
    "Køres i en temp directory. \n",
    "\n",
    "Kan specifier miljø variable.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Import the BashOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Define the BashOperator \n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='cleanup.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=analytics_dag\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh',\n",
    "    dag=analytics_dag)\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh',\n",
    "    dag = analytics_dag)\n",
    "\n",
    "```\n",
    "\n",
    "## Airflow tasks\n",
    "\n",
    "tasks er instanc eof operator.\n",
    "tilskrives en variable i python. \n",
    "\n",
    "Afhængigheder referes til som upstream eller downstream tasks. \n",
    "\n",
    "- Upstram betyder det skal gøres færdi prior til downstream tasks.\n",
    "\n",
    "bruger bitshift operator.\n",
    "\n",
    "Hvornår bruger man den ene\n",
    "Upstream er before\n",
    "downstream er after\n",
    "\n",
    "Der kan være mange afhængigheder\n",
    "\n",
    "Der kan være mixed afhængigheder. \n",
    "\n",
    "Kan definere mixed afhængigeder og upstream og downstream med `<<` og `>>`.\n",
    "\n",
    "```\n",
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command = 'wget https://salestracking/latestinfo?json',\n",
    "    dag=analytics_dag\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data\n",
    "```\n",
    "\n",
    "```\n",
    "-------------------------------------------------------------------\n",
    "DAGS\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "repl:~$ cat workspace/dags/codependent.py\n",
    "\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 2, 12),\n",
    "  'retries': 1\n",
    "}\n",
    "\n",
    "codependency_dag = DAG('codependency', default_args=default_args)\n",
    "\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     bash_command='echo 1',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "task2 = BashOperator(task_id='second_task',\n",
    "                     bash_command='echo 2',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "task3 = BashOperator(task_id='third_task',\n",
    "                     bash_command='echo 3',\n",
    "                     dag=codependency_dag)\n",
    "\n",
    "# task1 must run before task2 which must run before task3\n",
    "task1 >> task2\n",
    "task2 >> task3\n",
    "task3 >> task1\n",
    "repl:~$\n",
    "```\n",
    "\n",
    "## Additional operators\n",
    "\n",
    "PythonOperator køre en fuktion, men skal bruge samme kommdoer som i bashoperator.  Tildase argument til taks, positional og keyword. \n",
    "\n",
    "```\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable = parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    "    # Add the DAG\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "    \n",
    "```\n",
    "\n",
    "EmailOperator, sender en email med indhold. \n",
    "\n",
    "```\n",
    "# Import the Operator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task \n",
    "```\n",
    "\n",
    "De fleste ligger i `airflow.operator`\n",
    "\n",
    "\n",
    "## Airflow scheduling\n",
    "\n",
    "kan få en kørelse til et tidspunkt\n",
    "\n",
    "cron eksempler\n",
    "\n",
    "```\n",
    "* * 25 2 * # run once per minute on february 25\n",
    "\n",
    "@hourly         0 * * * *\n",
    "# etc.\n",
    "```\n",
    "\n",
    "None aldrig løb dag.\n",
    "@once kør dag engang. \n",
    "\n",
    "```\n",
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3') \n",
    "```\n",
    "\n",
    "# Maintaining and monitoring Airflow workflows\n",
    "\n",
    "## Sensors\n",
    "\n",
    "en type operator der afventer påm en sans. \n",
    "poke kører gentagende\n",
    "reschedule \n",
    "poke_interval checke rhovr ote\n",
    "\n",
    "file sensor er vigtig og ser om eksisent af en fil \n",
    "\n",
    "mange andre sensor\n",
    "ExternalTaskSensor\n",
    "\n",
    "## Airflow executer\n",
    "\n",
    "den der køre taks.\n",
    "\n",
    "der er en som hedder celeryexecutor\n",
    "\n",
    "default er sequentialexecutor god til debug ikke brug i produktion.\n",
    "airflow.cgf og find executor. \n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "## debuggin and troubleshooting in airflow\n",
    "\n",
    "# Building production pipelines in Airflow"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
